# -*- coding: utf-8 -*-
"""fake_news_model_pkl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iJk5IBbNX1XiqPa2Eb7NKye_2K5F2YN5

**Step-1: Load Your Dataset**
"""

import pandas as pd

df = pd.read_csv('/content/News_dataset.csv')
print(df.columns.tolist())

"""**Step-2: Preprocess the Text**"""

import os
os.makedirs('utils', exist_ok=True)

with open("utils/preprocess.py", "w") as f:
    f.write("""
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\\s]', '', text)
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return ' '.join(tokens)
""")

!pip uninstall -y nltk
!pip install nltk

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df = pd.read_csv('/content/News_dataset.csv')

df.columns = df.columns.str.strip().str.lower()

df = df.dropna(subset=['title', 'label'])

df['clean_title'] = df['title']

print(df[['title', 'clean_title']].head())

"""**Step-3: TF-IDF Vectorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['clean_title'])
y = df['label']

"""**Step-4: Train-Test Split**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""**Step-5: Train the classifier**"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

"""**Step-6: Evaluate the Model**"""

from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

"""**Step-7: Save the Model and Vectorizer**"""

import pickle
import os

os.makedirs('models', exist_ok=True)

with open('models/fake_news_model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('models/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

print("âœ… Model and vectorizer saved in models/")